{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Needed Libs"
      ],
      "metadata": {
        "id": "fVQsbseyLBFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is based on the github link :\n",
        "# https://github.com/mpezeshki/pytorch_forward_forward"
      ],
      "metadata": {
        "id": "ctCsaHkv8it9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow0Je4YEHSwN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating neg labeles"
      ],
      "metadata": {
        "id": "fLekJ5-OLotr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_y_neg(y):\n",
        "    y_neg = y.clone()\n",
        "    for idx, y_samp in enumerate(y):\n",
        "        allowed_indices = list(range(10))\n",
        "        allowed_indices.remove(y_samp.item())\n",
        "        y_neg[idx] = torch.tensor(allowed_indices)[\n",
        "            torch.randint(len(allowed_indices), size=(1,))\n",
        "        ].item()\n",
        "    return y_neg.to(device)\n",
        "\n",
        "\n",
        "def overlay_y_on_x(x, y, classes=10):\n",
        "    x_ = x.clone()\n",
        "    x_[:, :classes] *= 0.0\n",
        "    x_[range(x.shape[0]), y] = x.max()\n",
        "    return x_\n"
      ],
      "metadata": {
        "id": "ok_HjFwDH4br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base Layer class"
      ],
      "metadata": {
        "id": "pt1GsrJsLuU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(nn.Linear):\n",
        "    def __init__(self, in_features, out_features, bias=True, device=None, dtype=None):\n",
        "        super().__init__(in_features, out_features, bias, device, dtype)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.opt = Adam(self.parameters(), lr=learning_rate)\n",
        "        self.threshold = threshold\n",
        "        self.num_epochs = epochs\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
        "        return self.relu(torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0))\n",
        "\n",
        "    def train(self, x_pos, x_neg):\n",
        "        for i in range(self.num_epochs):\n",
        "            g_pos = self.forward(x_pos).pow(2).mean(1)\n",
        "            g_neg = self.forward(x_neg).pow(2).mean(1)\n",
        "            loss = torch.log(\n",
        "                1\n",
        "                + torch.exp(\n",
        "                    torch.cat([-g_pos + self.threshold, g_neg - self.threshold])\n",
        "                )\n",
        "            ).mean()\n",
        "            self.opt.zero_grad()\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "            if i % log_interval == 0:\n",
        "                print(\"Loss: \", loss.item())\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()\n"
      ],
      "metadata": {
        "id": "yGZXu34jLvPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based Net class"
      ],
      "metadata": {
        "id": "5o0MNlpIL4MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, dims):\n",
        "\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        for d in range(len(dims) - 1):\n",
        "            self.layers.append(Layer(dims[d], dims[d + 1]).to(device))\n",
        "\n",
        "    def predict(self, x):\n",
        "        goodness_per_label = []\n",
        "        for label in range(10):\n",
        "            h = overlay_y_on_x(x, label)\n",
        "            goodness = []\n",
        "            for layer in self.layers:\n",
        "                h = layer(h)\n",
        "                goodness.append(h.pow(2).mean(1))\n",
        "            goodness_per_label.append(sum(goodness).unsqueeze(1))\n",
        "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
        "        return goodness_per_label.argmax(1)\n",
        "\n",
        "    def train(self, x_pos, x_neg):\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            print(\"training layer: \", i)\n",
        "            h_pos, h_neg = layer.train(h_pos, h_neg)"
      ],
      "metadata": {
        "id": "Fea-o3IxL686"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the parameters and train the model"
      ],
      "metadata": {
        "id": "t3YmXSQ_L8Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpler variable assignments\n",
        "epochs = 1000\n",
        "learning_rate = 0.05\n",
        "no_cuda = False\n",
        "no_mps = False\n",
        "random_seed = 1234\n",
        "save_model = False\n",
        "train_size = 50000\n",
        "threshold = 2\n",
        "test_size = 10000\n",
        "log_interval = 10\n",
        "\n",
        "# Rest of the code remains unchanged\n",
        "\n",
        "# Using the variables\n",
        "use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "use_mps = not no_mps and torch.backends.mps.is_available()\n",
        "\n",
        "if use_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "elif use_mps:\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "train_kwargs = {\"batch_size\": train_size}\n",
        "test_kwargs = {\"batch_size\": test_size}\n",
        "\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "\n",
        "transform = Compose(\n",
        "    [\n",
        "        ToTensor(),\n",
        "        Normalize((0.1307,), (0.3081,)),\n",
        "        Lambda(lambda x: torch.flatten(x)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    MNIST(\"./data/\", train=True, download=True, transform=transform), **train_kwargs\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    MNIST(\"./data/\", train=False, download=True, transform=transform), **test_kwargs\n",
        ")\n"
      ],
      "metadata": {
        "id": "cPNs70J9H82x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling the Net class , Training the model, Giving back the accuracy"
      ],
      "metadata": {
        "id": "B54jUSDpM6me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net([784, 500, 500])\n",
        "\n",
        "x, y = next(iter(train_loader))\n",
        "x, y = x.to(device), y.to(device)\n",
        "x_pos = overlay_y_on_x(x, y)\n",
        "y_neg = get_y_neg(y)\n",
        "x_neg = overlay_y_on_x(x, y_neg)\n",
        "\n",
        "net.train(x_pos, x_neg)\n",
        "print(\"train error:\", 1.0 - net.predict(x).eq(y).float().mean().item())\n",
        "\n",
        "x_te, y_te = next(iter(test_loader))\n",
        "x_te, y_te = x_te.to(device), y_te.to(device)\n",
        "\n",
        "#if save_model:\n",
        "#    torch.save(net.state_dict(), \"mnist_ff.pt\")\n",
        "\n",
        "print(\"test error:\", 1.0 - net.predict(x_te).eq(y_te).float().mean().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxlDQ8jcM6Dv",
        "outputId": "1cce4a78-44d2-4e01-944b-4cd4435f0558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training layer:  0\n",
            "Loss:  1.126759648323059\n",
            "Loss:  0.7035413980484009\n",
            "Loss:  0.6998260021209717\n",
            "Loss:  0.6971884965896606\n",
            "Loss:  0.692277193069458\n",
            "Loss:  0.6863791942596436\n",
            "Loss:  0.6811739802360535\n",
            "Loss:  0.6726803779602051\n",
            "Loss:  0.6613404750823975\n",
            "Loss:  0.6469516754150391\n",
            "Loss:  0.6306005120277405\n",
            "Loss:  0.6136147975921631\n",
            "Loss:  0.5969079732894897\n",
            "Loss:  0.5807739496231079\n",
            "Loss:  0.5652912855148315\n",
            "Loss:  0.5504934191703796\n",
            "Loss:  0.5363852977752686\n",
            "Loss:  0.5229905843734741\n",
            "Loss:  0.5103220343589783\n",
            "Loss:  0.4983688294887543\n",
            "Loss:  0.48709607124328613\n",
            "Loss:  0.47646650671958923\n",
            "Loss:  0.4664372205734253\n",
            "Loss:  0.45696982741355896\n",
            "Loss:  0.4480300545692444\n",
            "Loss:  0.43958255648612976\n",
            "Loss:  0.4315928518772125\n",
            "Loss:  0.42402708530426025\n",
            "Loss:  0.4168485105037689\n",
            "Loss:  0.41002514958381653\n",
            "Loss:  0.4035312831401825\n",
            "Loss:  0.3973410129547119\n",
            "Loss:  0.3914314806461334\n",
            "Loss:  0.38578560948371887\n",
            "Loss:  0.3803861737251282\n",
            "Loss:  0.37521445751190186\n",
            "Loss:  0.3702540397644043\n",
            "Loss:  0.3654909133911133\n",
            "Loss:  0.36091265082359314\n",
            "Loss:  0.35650694370269775\n",
            "Loss:  0.3522617816925049\n",
            "Loss:  0.3481649160385132\n",
            "Loss:  0.34420546889305115\n",
            "Loss:  0.34037452936172485\n",
            "Loss:  0.3366640508174896\n",
            "Loss:  0.3330672085285187\n",
            "Loss:  0.329576700925827\n",
            "Loss:  0.3261851966381073\n",
            "Loss:  0.3228853642940521\n",
            "Loss:  0.31967055797576904\n",
            "Loss:  0.3165349066257477\n",
            "Loss:  0.31347334384918213\n",
            "Loss:  0.3104817867279053\n",
            "Loss:  0.3075568974018097\n",
            "Loss:  0.3046961724758148\n",
            "Loss:  0.30189740657806396\n",
            "Loss:  0.29915833473205566\n",
            "Loss:  0.2964771091938019\n",
            "Loss:  0.29385262727737427\n",
            "Loss:  0.2912832200527191\n",
            "Loss:  0.2887667715549469\n",
            "Loss:  0.28630101680755615\n",
            "Loss:  0.2838834822177887\n",
            "Loss:  0.28151190280914307\n",
            "Loss:  0.2791839838027954\n",
            "Loss:  0.27689769864082336\n",
            "Loss:  0.27465128898620605\n",
            "Loss:  0.2724430561065674\n",
            "Loss:  0.2702713906764984\n",
            "Loss:  0.2681346535682678\n",
            "Loss:  0.26603132486343384\n",
            "Loss:  0.26396042108535767\n",
            "Loss:  0.26192131638526917\n",
            "Loss:  0.25991350412368774\n",
            "Loss:  0.25793635845184326\n",
            "Loss:  0.2559892237186432\n",
            "Loss:  0.25407135486602783\n",
            "Loss:  0.2521823048591614\n",
            "Loss:  0.25032225251197815\n",
            "Loss:  0.24849198758602142\n",
            "Loss:  0.24669192731380463\n",
            "Loss:  0.24492192268371582\n",
            "Loss:  0.2431812882423401\n",
            "Loss:  0.24146917462348938\n",
            "Loss:  0.2397848218679428\n",
            "Loss:  0.23812752962112427\n",
            "Loss:  0.23649679124355316\n",
            "Loss:  0.2348918616771698\n",
            "Loss:  0.23331217467784882\n",
            "Loss:  0.23175697028636932\n",
            "Loss:  0.23022538423538208\n",
            "Loss:  0.22871655225753784\n",
            "Loss:  0.22722962498664856\n",
            "Loss:  0.22576364874839783\n",
            "Loss:  0.22431784868240356\n",
            "Loss:  0.2228914350271225\n",
            "Loss:  0.22148394584655762\n",
            "Loss:  0.22009500861167908\n",
            "Loss:  0.2187243551015854\n",
            "Loss:  0.21737155318260193\n",
            "training layer:  1\n",
            "Loss:  1.1266875267028809\n",
            "Loss:  0.5096294283866882\n",
            "Loss:  0.5159502029418945\n",
            "Loss:  0.47624319791793823\n",
            "Loss:  0.45547857880592346\n",
            "Loss:  0.4304226338863373\n",
            "Loss:  0.40498268604278564\n",
            "Loss:  0.38093990087509155\n",
            "Loss:  0.36084967851638794\n",
            "Loss:  0.34386786818504333\n",
            "Loss:  0.3295785188674927\n",
            "Loss:  0.3174108564853668\n",
            "Loss:  0.3069577217102051\n",
            "Loss:  0.2978915274143219\n",
            "Loss:  0.289933443069458\n",
            "Loss:  0.2828640341758728\n",
            "Loss:  0.2765227258205414\n",
            "Loss:  0.27078181505203247\n",
            "Loss:  0.26553651690483093\n",
            "Loss:  0.26070940494537354\n",
            "Loss:  0.25623658299446106\n",
            "Loss:  0.25207650661468506\n",
            "Loss:  0.24819236993789673\n",
            "Loss:  0.2445506900548935\n",
            "Loss:  0.24111735820770264\n",
            "Loss:  0.237870991230011\n",
            "Loss:  0.2347884327173233\n",
            "Loss:  0.23186533153057098\n",
            "Loss:  0.22910627722740173\n",
            "Loss:  0.22649796307086945\n",
            "Loss:  0.22402523458003998\n",
            "Loss:  0.22167499363422394\n",
            "Loss:  0.219436377286911\n",
            "Loss:  0.21730031073093414\n",
            "Loss:  0.21525883674621582\n",
            "Loss:  0.21330472826957703\n",
            "Loss:  0.21143165230751038\n",
            "Loss:  0.20963436365127563\n",
            "Loss:  0.20790773630142212\n",
            "Loss:  0.20624710619449615\n",
            "Loss:  0.20464825630187988\n",
            "Loss:  0.20310717821121216\n",
            "Loss:  0.20162011682987213\n",
            "Loss:  0.2001837193965912\n",
            "Loss:  0.19879503548145294\n",
            "Loss:  0.1974516212940216\n",
            "Loss:  0.19615104794502258\n",
            "Loss:  0.19489112496376038\n",
            "Loss:  0.19366972148418427\n",
            "Loss:  0.19248491525650024\n",
            "Loss:  0.19133491814136505\n",
            "Loss:  0.1902180165052414\n",
            "Loss:  0.18913264572620392\n",
            "Loss:  0.18807724118232727\n",
            "Loss:  0.18705040216445923\n",
            "Loss:  0.18605086207389832\n",
            "Loss:  0.18507733941078186\n",
            "Loss:  0.1841287910938263\n",
            "Loss:  0.1832040250301361\n",
            "Loss:  0.1823020726442337\n",
            "Loss:  0.18142195045948029\n",
            "Loss:  0.18056276440620422\n",
            "Loss:  0.17972368001937866\n",
            "Loss:  0.17890384793281555\n",
            "Loss:  0.17810246348381042\n",
            "Loss:  0.17731884121894836\n",
            "Loss:  0.17655238509178162\n",
            "Loss:  0.17580242455005646\n",
            "Loss:  0.17506840825080872\n",
            "Loss:  0.17434969544410706\n",
            "Loss:  0.1736457794904709\n",
            "Loss:  0.17295609414577484\n",
            "Loss:  0.1722801774740219\n",
            "Loss:  0.17161749303340912\n",
            "Loss:  0.17096765339374542\n",
            "Loss:  0.17033015191555023\n",
            "Loss:  0.1697046011686325\n",
            "Loss:  0.1690906137228012\n",
            "Loss:  0.16848783195018768\n",
            "Loss:  0.1678958386182785\n",
            "Loss:  0.1673143059015274\n",
            "Loss:  0.16674289107322693\n",
            "Loss:  0.16618125140666962\n",
            "Loss:  0.16562917828559875\n",
            "Loss:  0.1650862842798233\n",
            "Loss:  0.16455230116844177\n",
            "Loss:  0.1640268713235855\n",
            "Loss:  0.16350969672203064\n",
            "Loss:  0.16300053894519806\n",
            "Loss:  0.1624990999698639\n",
            "Loss:  0.1620052307844162\n",
            "Loss:  0.16151879727840424\n",
            "Loss:  0.16103966534137726\n",
            "Loss:  0.16056765615940094\n",
            "Loss:  0.16010259091854095\n",
            "Loss:  0.15964427590370178\n",
            "Loss:  0.1591925024986267\n",
            "Loss:  0.15874704718589783\n",
            "Loss:  0.15830768644809723\n",
            "Loss:  0.15787428617477417\n",
            "train error: 0.06056004762649536\n",
            "test error: 0.06150001287460327\n"
          ]
        }
      ]
    }
  ]
}