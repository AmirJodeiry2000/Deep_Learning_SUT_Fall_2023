{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHJ0Q3pPfyPk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelWithLMHead\n",
        "import gdown\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwZHpHuSJUX"
      },
      "source": [
        "## Extracting txt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk8sUI7vSHIV",
        "outputId": "76c3e0fe-5603-4ef9-ac01-31dbb069f607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Ferdowsi_Poetry_Composer' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AmirJodeiry2000/Deep_Learning_SUT_Fall_2023.git Ferdowsi_Poetry_Composer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-TTq4aHSyGk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_directory = '/content/Ferdowsi_Poetry_Composer'\n",
        "\n",
        "file_path = os.path.join(current_directory, 'Ferdowsi_Poetry_Composer', 'ferdousi.txt')\n",
        "\n",
        "# Open and read the content of the ferdousi.txt file\n",
        "with open(file_path, 'r') as file:\n",
        "    ferdowsi_poems = file.read()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh8TaKqdUwYI"
      },
      "source": [
        "## Dividing into verses.\n",
        "## verse1 input , verse1 ouptut\n",
        "## the same for both of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-8xWeqIiMCP",
        "outputId": "04910e5f-79cc-48a2-e1b9-ff10d58ce86f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "از آن پس نمیرم که من زنده ام    که تخم سخن من پراگنده ام\n"
          ]
        }
      ],
      "source": [
        "\n",
        "input_verses = []\n",
        "output_verses = []\n",
        "poem_lines = ferdowsi_poems.strip().split('\\n')\n",
        "\n",
        "for i in range(2, len(poem_lines) - 2, 2):\n",
        "    input_poetry = '    '.join([line    for line in poem_lines[i:i+2]])\n",
        "    output_poetry = input_poetry\n",
        "    input_verses.append(input_poetry)\n",
        "    output_verses.append(output_poetry)\n",
        "\n",
        "print(input_verses[-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding model name and Tokenizer and it's considerations"
      ],
      "metadata": {
        "id": "IA5yT1HnsA7T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmRKAJhHT01j",
        "outputId": "41f76a3a-8fd0-4099-e714-a5b1fe38cf49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-20 14:48:22--  https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/pytorch_model.bin\n",
            "Resolving huggingface.co (huggingface.co)... 65.9.86.79, 65.9.86.62, 65.9.86.71, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.9.86.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/HooshvareLab/gpt2-fa/46b0b806c740a0f0a9f056f5574c5fa896166fe844945fd3c849bf34365e5060?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1706021302&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjAyMTMwMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9Ib29zaHZhcmVMYWIvZ3B0Mi1mYS80NmIwYjgwNmM3NDBhMGYwYTlmMDU2ZjU1NzRjNWZhODk2MTY2ZmU4NDQ5NDVmZDNjODQ5YmYzNDM2NWU1MDYwP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=Db0BOJIQ1WF1cV19XONXKrAsV-MGgLecXQdbCgIctupwGQr2K5ftJOLSX-usYmhiZfjCG3-82Q7FBhHZR1C86oyWDvC2vrPYWTn2h86Dkn30oNwOJv86H%7EeJFm%7EKZCT0tVM6qM4OfMnS1rZl47C0slBzgkAgu7CFBC-yhrenj5pNnh-qP%7EGjR54CDC4zVVZBPpHH09g8Qy03KV5%7E4X73j57-MrDfyA5xW55SF1I0O4YVnTD1KQ-JCn3NFTRj59hFfbsuX1xPFh3c--HD5vdcvULneHRE9dAS8uWk9vbbGvK-kL7GvtHM4brKQkGNaneBmKtw2pl0ZM45byRsmLCdfw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-01-20 14:48:22--  https://cdn-lfs.huggingface.co/HooshvareLab/gpt2-fa/46b0b806c740a0f0a9f056f5574c5fa896166fe844945fd3c849bf34365e5060?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1706021302&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjAyMTMwMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9Ib29zaHZhcmVMYWIvZ3B0Mi1mYS80NmIwYjgwNmM3NDBhMGYwYTlmMDU2ZjU1NzRjNWZhODk2MTY2ZmU4NDQ5NDVmZDNjODQ5YmYzNDM2NWU1MDYwP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=Db0BOJIQ1WF1cV19XONXKrAsV-MGgLecXQdbCgIctupwGQr2K5ftJOLSX-usYmhiZfjCG3-82Q7FBhHZR1C86oyWDvC2vrPYWTn2h86Dkn30oNwOJv86H%7EeJFm%7EKZCT0tVM6qM4OfMnS1rZl47C0slBzgkAgu7CFBC-yhrenj5pNnh-qP%7EGjR54CDC4zVVZBPpHH09g8Qy03KV5%7E4X73j57-MrDfyA5xW55SF1I0O4YVnTD1KQ-JCn3NFTRj59hFfbsuX1xPFh3c--HD5vdcvULneHRE9dAS8uWk9vbbGvK-kL7GvtHM4brKQkGNaneBmKtw2pl0ZM45byRsmLCdfw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.239.18.84, 18.239.18.94, 18.239.18.68, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.239.18.84|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 485044198 (463M) [application/octet-stream]\n",
            "Saving to: ‘/content/gpt2/pytorch_model.bin.2’\n",
            "\n",
            "pytorch_model.bin.2 100%[===================>] 462.57M  68.9MB/s    in 4.4s    \n",
            "\n",
            "2024-01-20 14:48:27 (104 MB/s) - ‘/content/gpt2/pytorch_model.bin.2’ saved [485044198/485044198]\n",
            "\n",
            "--2024-01-20 14:48:27--  https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/tokenizer.json\n",
            "Resolving huggingface.co (huggingface.co)... 65.9.86.79, 65.9.86.62, 65.9.86.71, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.9.86.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2748949 (2.6M) [text/plain]\n",
            "Saving to: ‘/content/gpt2/tokenizer.json.3’\n",
            "\n",
            "tokenizer.json.3    100%[===================>]   2.62M  5.38MB/s    in 0.5s    \n",
            "\n",
            "2024-01-20 14:48:28 (5.38 MB/s) - ‘/content/gpt2/tokenizer.json.3’ saved [2748949/2748949]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_name_or_path = \"HooshvareLab/gpt2-fa\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    bos_token='<s>',\n",
        "    eos_token='<eos>',\n",
        "    pad_token='<pad>',\n",
        "    unk_token='<unk>'\n",
        ")\n",
        "tokenizer.add_special_tokens({\n",
        "    \"bos_token\": '<s>',\n",
        "    \"eos_token\": '<eos>',\n",
        "    \"pad_token\": '<pad>',\n",
        "    \"unk_token\": '<unk>'\n",
        "})\n",
        "config = AutoConfig.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    bos_token_id=tokenizer(\"<s>\")[\"input_ids\"][0],\n",
        "    eos_token_id=tokenizer(\"<eos>\")[\"input_ids\"][0],\n",
        "    pad_token_id=tokenizer(\"<pad>\")[\"input_ids\"][0],\n",
        "    unk_token_id=tokenizer(\"<unk>\")[\"input_ids\"][0],\n",
        ")\n",
        "\n",
        "tokenizer.save_pretrained(\"/content/gpt2/\")\n",
        "config.save_pretrained(\"/content/gpt2/\")\n",
        "\n",
        "!wget \"https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/pytorch_model.bin\" -P /content/gpt2/\n",
        "!wget \"https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/tokenizer.json\" -P /content/gpt2/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev-I0wS6iaPE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b0c9b1-75ef-49f1-a2ca-45631ef79740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"/content/gpt2\",\n",
        "    bos_token='<s>',\n",
        "    eos_token='<eos>',\n",
        "    pad_token='<pad>'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Max seq of tokenized data"
      ],
      "metadata": {
        "id": "8MGyLz3xsIQd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BEHxqkCijle",
        "outputId": "d00d8c9c-6a52-4a7c-c5a3-8338f3f06984"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "max_seq = max([len(tokenizer.encode(text)) for text in input_verses])\n",
        "max_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Max seq of verses"
      ],
      "metadata": {
        "id": "H8jL__jnsNsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq = max(len(poem) for poem in input_verses)\n",
        "max_seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72_kEYG3AVF6",
        "outputId": "1256b326-4989-482d-ddc1-017aec61f14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Writing A class for Dataset just to applying tokenization and returning input id and attention mask"
      ],
      "metadata": {
        "id": "JhFEa_IdsUqR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIQnJQ1TjWhe"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "class MTGDataset(Dataset):\n",
        "\n",
        "    def __init__(self, txt_list, tokenizer, max_length=1024):\n",
        "\n",
        "        self.tokenizer = tokenizer  # the gpt2 tokenizer we instantiated\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "\n",
        "        for txt in txt_list:\n",
        "            \"\"\"\n",
        "            This loop will iterate through each entry in the flavour text corpus.\n",
        "            For each bit of text it will prepend it with the start of text token,\n",
        "            then append the end of text token and pad to the maximum length with the\n",
        "            pad token.\n",
        "            \"\"\"\n",
        "\n",
        "            encodings_dict = tokenizer('<s>' + txt + '<eos>',\n",
        "                                       truncation=True,\n",
        "                                       max_length=max_length,\n",
        "                                       padding=\"max_length\")\n",
        "\n",
        "            \"\"\"\n",
        "            Each iteration then appends either the encoded tensor to a list,\n",
        "            or the attention mask for that encoding to a list. The attention mask is\n",
        "            a binary list of 1's or 0's which determine whether the langauge model\n",
        "            should take that token into consideration or not.\n",
        "            \"\"\"\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating tokenized input and ouptut ( the same )"
      ],
      "metadata": {
        "id": "AKt8eM4Hsdcs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LShY2Fv5jbV1"
      },
      "outputs": [],
      "source": [
        "tokenized_input = MTGDataset(input_verses, tokenizer, max_length=max_seq)\n",
        "tokenized_output = MTGDataset(output_verses, tokenizer, max_length=max_seq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocab size which will be used then"
      ],
      "metadata": {
        "id": "o7w0tvfAsiuU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU9AoxDnQ9YO",
        "outputId": "7493defe-8bd5-4201-9baa-042fe364e32e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42002"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "vocab_size = len(tokenizer)\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mx-0JjFwmQ8",
        "outputId": "36b667f2-aa2e-4856-bac3-d145af3768e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([    0,  2984,   297, 12519,  1204,   293,  1305,   293,  1079, 21147,\n",
              "         17801, 37144,   330,  2496,  1010, 42001,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1]),\n",
              " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tokenized_input[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0SFH3cuZxKV"
      },
      "source": [
        "## Spliting into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zoEn9y_EY9O"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "\n",
        "input_ids = tokenized_input.input_ids\n",
        "labels = tokenized_output.input_ids\n",
        "attn_masks_input = tokenized_input.attn_masks\n",
        "attn_masks_output = tokenized_output.attn_masks\n",
        "\n",
        "\n",
        "train_input_ids, test_input_ids, train_labels, test_labels, train_attn_masks, test_attn_masks = train_test_split(\n",
        "    input_ids, labels, attn_masks_input, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_input_ids = torch.stack(train_input_ids)\n",
        "test_input_ids = torch.stack(test_input_ids)\n",
        "train_labels = torch.stack(train_labels)\n",
        "test_labels = torch.stack(test_labels)\n",
        "train_attention_mask = torch.stack(train_attn_masks)\n",
        "test_attention_mask = torch.stack(test_attn_masks)\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X250064GaCZU"
      },
      "source": [
        "## Building Dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The max batch size is 32. more than that needs more poewerfull GPUs"
      ],
      "metadata": {
        "id": "c0DEMSjisnx0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsxfgMXlJK7r"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calling the model and config using the causalLm Architecture"
      ],
      "metadata": {
        "id": "emB05ZszswQo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_77qQjhaJPc",
        "outputId": "18b6c0e9-faf6-4dad-c68d-ac05c849145b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoConfig\n",
        "\n",
        "model_name_or_path = \"HooshvareLab/gpt2-fa\"\n",
        "\n",
        "configuration = GPT2Config.from_pretrained(model_name_or_path, output_hidden_states=False)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, config=configuration)\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting the drive just for saving the model."
      ],
      "metadata": {
        "id": "AA7fJfGqs3Ap"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2xxH0P6--FQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c15bfb3-822a-4c42-e124-d73fc9ab1ace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_save_path = \"/content/drive/MyDrive/model_ferdowsi_same_verses.pt\"\n",
        "# model.load_state_dict(torch.load(model_save_path))\n"
      ],
      "metadata": {
        "id": "CWqggMUiXBLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding optimizer and criterion ( model has it's own criterion (CE))"
      ],
      "metadata": {
        "id": "6UcmHq0Vs8v-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7tY-KqgbLX1"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr = 1e-4\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRqu12gddUQ7"
      },
      "source": [
        "## Train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLKXl3wPz8Eg",
        "outputId": "95a9de36-e03a-4a58-ba6d-e9ae2f5c9e72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM7_yX8izp12",
        "outputId": "283c5975-10a3-44ff-c671-0a090093edef"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.1417034618994768\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 2.5954205989837646\n",
            "Test Loss: 0.9537485772773767\n",
            "Model saved at: /content/drive/MyDrive/model_ferdowsi_same_verses.pt IN EPOCH\n",
            "\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.8955685225443721\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 2.380176544189453\n",
            "Test Loss: 0.8671746809766223\n",
            "Model saved at: /content/drive/MyDrive/model_ferdowsi_same_verses.pt IN EPOCH\n",
            "\n",
            "Epoch 3/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.7916613266316275\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 2.2908239364624023\n",
            "Test Loss: 0.8289115722156414\n",
            "Model saved at: /content/drive/MyDrive/model_ferdowsi_same_verses.pt IN EPOCH\n",
            "\n",
            "Epoch 4/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.7135632723978121\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 2.267760753631592\n",
            "Test Loss: 0.8187928500666113\n",
            "Model saved at: /content/drive/MyDrive/model_ferdowsi_same_verses.pt IN EPOCH\n",
            "\n",
            "Epoch 5/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.6443250133662719\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 2.2699265480041504\n",
            "Test Loss: 0.8197474908982059\n",
            "Model saved at: /content/drive/MyDrive/model_ferdowsi_same_verses.pt IN EPOCH\n",
            "\n",
            "Epoch 6/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.5800133444050644\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 2.297515630722046\n",
            "Test Loss: 0.8318283322156433\n",
            "Model saved at: /content/drive/MyDrive/model_ferdowsi_same_verses.pt IN EPOCH\n",
            "\n",
            "Epoch 7/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.520969351821134\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 2.340404510498047\n",
            "Test Loss: 0.8503237460587185\n",
            "Model saved at: /content/drive/MyDrive/model_ferdowsi_same_verses.pt IN EPOCH\n",
            "\n",
            "Epoch 8/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.46664725917659394\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 2.3923964500427246\n",
            "Test Loss: 0.8722955571110226\n",
            "Model saved at: /content/drive/MyDrive/model_ferdowsi_same_verses.pt IN EPOCH\n",
            "\n",
            "Epoch 9/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.4175567725413658\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 2.4525020122528076\n",
            "Test Loss: 0.8971087267544492\n",
            "Model saved at: /content/drive/MyDrive/model_ferdowsi_same_verses.pt IN EPOCH\n",
            "\n",
            "Epoch 10/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.37550696572116266\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 2.5125253200531006\n",
            "Test Loss: 0.921288293465924\n",
            "Model saved at: /content/drive/MyDrive/model_ferdowsi_same_verses.pt IN EPOCH\n",
            "\n",
            "Epoch 11/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.33986385528067253\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 2.585261583328247\n",
            "Test Loss: 0.9498267156518158\n",
            "Model saved at: /content/drive/MyDrive/model_ferdowsi_same_verses.pt IN EPOCH\n",
            "\n",
            "Epoch 12/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.308524270123959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 2.6525380611419678\n",
            "Test Loss: 0.9755169417697134\n",
            "Model saved at: /content/drive/MyDrive/model_ferdowsi_same_verses.pt IN EPOCH\n",
            "\n",
            "Epoch 13/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.28457875069497957\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 2.7161295413970947\n",
            "Test Loss: 0.9992079010347078\n",
            "Model saved at: /content/drive/MyDrive/model_ferdowsi_same_verses.pt IN EPOCH\n",
            "\n",
            "Epoch 14/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2661922644046312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Perplexity: 2.7857611179351807\n",
            "Test Loss: 1.0245211143585646\n",
            "Model saved at: /content/drive/MyDrive/model_ferdowsi_same_verses.pt IN EPOCH\n",
            "\n",
            "Epoch 15/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2521922831017581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Perplexity: 2.8236982822418213\n",
            "Test Loss: 1.0380474097092436\n",
            "Model saved at: /content/drive/MyDrive/model_ferdowsi_same_verses.pt IN EPOCH\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss=outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    return avg_loss\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\", leave=False):\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss=outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += labels.numel()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "    return avg_loss,perplexity.item()\n",
        "\n",
        "\n",
        "num_epochs = 15\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    print(f\"Train Loss: {train_loss:}\")\n",
        "\n",
        "    test_loss,test_perplexity  = evaluate(model, test_loader, criterion, device)\n",
        "    print(f'Test Perplexity: {test_perplexity}')\n",
        "    print(f\"Test Loss: {test_loss:}\")\n",
        "\n",
        "    model_save_path = \"/content/drive/MyDrive/model_ferdowsi_same_verses.pt\"\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved at: {model_save_path} IN EPOCH\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkH6pN7BG0VH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from IPython import display\n",
        "import pandas as pd\n",
        "\n",
        "def generator(poet, max_length=max_seq, num_return_sequences=3,temperature = 0.9):\n",
        "    model.eval()\n",
        "    print(prompt)\n",
        "\n",
        "    input_ids = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    decoded_outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=20,\n",
        "        num_beams=10,\n",
        "        no_repeat_ngram_size=2,\n",
        "        top_k=150,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        attention_mask=torch.ones_like(input_ids),\n",
        "        num_return_sequences =num_return_sequences\n",
        "        )\n",
        "\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    for i, output in enumerate(decoded_outputs):\n",
        "        o = tokenizer.decode(output, skip_special_tokens=False)\n",
        "        o = o.replace(\"<s>\", \"\").replace(\"<eos>\", \"\\n\")\n",
        "        outputs.append(o)\n",
        "\n",
        "\n",
        "\n",
        "    display.display(display.HTML(\"\"\"\n",
        "    <style>\n",
        "    @import url(\"https://cdn.jsdelivr.net/gh/rastikerdar/vazir-font@v27.1.0/dist/font-face.css\");\n",
        "\n",
        "    table.xxx {\n",
        "        margin-right: 15px;\n",
        "        font-size: 14px;\n",
        "        direction: rtl !important;\n",
        "        width: 100%;\n",
        "        display: flex;\n",
        "    }\n",
        "    table.xxx td {\n",
        "        min-width: 300px !important;\n",
        "        direction: rtl !important;\n",
        "        text-align: right !important;\n",
        "        font-family: \"Vazir\" !important;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\".strip()))\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(outputs, columns=[\"generated\"])\n",
        "    df[\"generated\"] = df[\"generated\"].apply(lambda t: re.sub(\"\\n+\", \"\\n\", t.replace(\"    \", \"\\n\")))\n",
        "    df[\"generated\"] = df[\"generated\"].apply(lambda t: \"<p>\" + t.replace(\"\\n\", \"<br/>\").replace(\"<|startoftext|>\", \"<br/>\").strip() + \"</p>\")\n",
        "\n",
        "\n",
        "    setup = {\n",
        "        'border': 2,\n",
        "        'show_dimensions': True,\n",
        "        'escape': False,\n",
        "        'justify': 'right',\n",
        "        'classes': 'xxx'\n",
        "    }\n",
        "    display.display(display.HTML(df.to_html(**setup)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some of the output samples"
      ],
      "metadata": {
        "id": "HROSIWgXtJAm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76lolu3xG9BD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "outputId": "1c0afc96-0101-46b1-b3e0-718d4bca7979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تو نیکی می کن و\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `150` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    @import url(\"https://cdn.jsdelivr.net/gh/rastikerdar/vazir-font@v27.1.0/dist/font-face.css\");\n",
              "\n",
              "    table.xxx {\n",
              "        margin-right: 15px;\n",
              "        font-size: 14px;\n",
              "        direction: rtl !important;\n",
              "        width: 100%;\n",
              "        display: flex;\n",
              "    }\n",
              "    table.xxx td {\n",
              "        min-width: 300px !important;\n",
              "        direction: rtl !important;\n",
              "        text-align: right !important;\n",
              "        font-family: \"Vazir\" !important;\n",
              "    }\n",
              "    </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"2\" class=\"dataframe xxx\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>generated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td><p>تو نیکی می کن و جام می برگزین<br/>که من می خور و رامش گزین<br/><pad><pad></p></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td><p>تو نیکی می کن و جام و می خواستی<br/>به می رامش و شادی بیاراستی<br/><pad></p></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1 columns</p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "توانا بود هر که\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `150` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    @import url(\"https://cdn.jsdelivr.net/gh/rastikerdar/vazir-font@v27.1.0/dist/font-face.css\");\n",
              "\n",
              "    table.xxx {\n",
              "        margin-right: 15px;\n",
              "        font-size: 14px;\n",
              "        direction: rtl !important;\n",
              "        width: 100%;\n",
              "        display: flex;\n",
              "    }\n",
              "    table.xxx td {\n",
              "        min-width: 300px !important;\n",
              "        direction: rtl !important;\n",
              "        text-align: right !important;\n",
              "        font-family: \"Vazir\" !important;\n",
              "    }\n",
              "    </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"2\" class=\"dataframe xxx\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>generated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td><p>توانا بود هر که خشنود باشد به گنج<br/>نیازد نیارد تنش را به رنج<br/><pad></p></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td><p>توانا بود هر که خشنود باشد به گنج<br/>نیازد نیارد گذشتن به رنج<br/><pad><pad></p></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1 columns</p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "سالی که نکوست\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `150` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    @import url(\"https://cdn.jsdelivr.net/gh/rastikerdar/vazir-font@v27.1.0/dist/font-face.css\");\n",
              "\n",
              "    table.xxx {\n",
              "        margin-right: 15px;\n",
              "        font-size: 14px;\n",
              "        direction: rtl !important;\n",
              "        width: 100%;\n",
              "        display: flex;\n",
              "    }\n",
              "    table.xxx td {\n",
              "        min-width: 300px !important;\n",
              "        direction: rtl !important;\n",
              "        text-align: right !important;\n",
              "        font-family: \"Vazir\" !important;\n",
              "    }\n",
              "    </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"2\" class=\"dataframe xxx\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>generated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td><p>سالی که نکوست بر ما زمان و زمین<br/>گذر کرد باید به دریای چین<br/><pad><pad></p></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td><p>سالی که نکوست بر ما زمان و زمین<br/>گذر کرد باید بداد آفرین<br/><pad><pad></p></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1 columns</p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "چنین داد پاسخ  که نیک \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `150` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    @import url(\"https://cdn.jsdelivr.net/gh/rastikerdar/vazir-font@v27.1.0/dist/font-face.css\");\n",
              "\n",
              "    table.xxx {\n",
              "        margin-right: 15px;\n",
              "        font-size: 14px;\n",
              "        direction: rtl !important;\n",
              "        width: 100%;\n",
              "        display: flex;\n",
              "    }\n",
              "    table.xxx td {\n",
              "        min-width: 300px !important;\n",
              "        direction: rtl !important;\n",
              "        text-align: right !important;\n",
              "        font-family: \"Vazir\" !important;\n",
              "    }\n",
              "    </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"2\" class=\"dataframe xxx\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>generated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td><p>چنین داد پاسخ  که نیک یران سپهر<br/>ببینیم تا بر که گردد بمهر<br/><pad><pad></p></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td><p>چنین داد پاسخ  که نیک یرانم<br/>هم از تخم شاهی و هم شیرم<br/><pad></p></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1 columns</p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = \"تو نیکی می کن و\"\n",
        "generator(prompt, num_return_sequences =2)\n",
        "prompt = \"توانا بود هر که\"\n",
        "generator(prompt, num_return_sequences =2)\n",
        "prompt = \"سالی که نکوست\"\n",
        "generator(prompt, num_return_sequences =2)\n",
        "prompt = \"چنین داد پاسخ  که نیک \"\n",
        "generator(prompt, num_return_sequences =2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}